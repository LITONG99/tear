2026-01-06 21:38:28 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_ours_large', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=3, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=8000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, min_train_epoch=30, model_parallel_size=1, newline_token='\n', no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=16, optimizer='adam', optimizer_overrides='{}', patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/export/data/tlice/schema_data/bart.large/model.pt', return_relative_column_strs=['col_head'], save_dir='/export/data/tlice/schema_data/tear/surrogate_model/Weather_1_2025', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2025, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='text', split_token='|', stop_time_hours=0, table_max_columns=20, target_lang='data', task='text_to_table_task', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=True, unconstrained_decoding=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='src/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=400, weight_decay=0.01, zero_sharding='none')
2026-01-06 21:38:28 | INFO | fairseq.tasks.translation | [text] dictionary: 50264 types
2026-01-06 21:38:28 | INFO | fairseq.tasks.translation | [data] dictionary: 50264 types
2026-01-06 21:38:30 | INFO | fairseq.data.data_utils | loaded 100 examples from: /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins/valid.text-data.text
2026-01-06 21:38:30 | INFO | fairseq.data.data_utils | loaded 100 examples from: /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins/valid.text-data.data
2026-01-06 21:38:30 | INFO | src.tasks.text_to_table_task | /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins valid text-data 100 examples
2026-01-06 21:38:38 | INFO | fairseq_cli.train | BARTOurs(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerOursDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2026-01-06 21:38:38 | INFO | fairseq_cli.train | task: text_to_table_task (TextToDataTranslationTask)
2026-01-06 21:38:38 | INFO | fairseq_cli.train | model: bart_ours_large (BARTOurs)
2026-01-06 21:38:38 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2026-01-06 21:38:38 | INFO | fairseq_cli.train | num. model params: 406293504 (num. trained: 406293504)
2026-01-06 21:38:50 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2026-01-06 21:38:50 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2026-01-06 21:38:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2026-01-06 21:38:50 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.252 GB ; name = NVIDIA A800 80GB PCIe                   
2026-01-06 21:38:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2026-01-06 21:38:50 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2026-01-06 21:38:50 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2026-01-06 21:39:31 | INFO | fairseq.trainer | loaded checkpoint /export/data/tlice/schema_data/bart.large/model.pt (epoch 41 @ 0 updates)
2026-01-06 21:39:31 | INFO | fairseq.trainer | loading train data for epoch 1
2026-01-06 21:39:31 | INFO | fairseq.data.data_utils | loaded 200 examples from: /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins/train.text-data.text
2026-01-06 21:39:31 | INFO | fairseq.data.data_utils | loaded 200 examples from: /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins/train.text-data.data
2026-01-06 21:39:31 | INFO | src.tasks.text_to_table_task | /export/data/tlice/schema_data/tear/surrogate_model/surrogate_data/Weather_1/bins train text-data 200 examples
2026-01-06 21:39:31 | INFO | fairseq.trainer | begin training epoch 1
2026-01-06 21:39:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2026-01-06 21:39:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2026-01-06 21:39:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2026-01-06 21:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:39:48 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 19.951 | nll_loss 18.637 | ppl 407750 | wps 7534 | wpb 1513.5 | bsz 50 | num_updates 0
2026-01-06 21:39:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2026-01-06 21:39:48 | INFO | train | epoch 001 | lr 1e-07 | loss_scale 16 | train_wall 15 | wall 58
2026-01-06 21:39:48 | INFO | fairseq.trainer | begin training epoch 2
2026-01-06 21:39:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2026-01-06 21:39:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2026-01-06 21:39:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2026-01-06 21:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:39:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 19.951 | nll_loss 18.637 | ppl 407750 | wps 8827.3 | wpb 1513.5 | bsz 50 | num_updates 0
2026-01-06 21:39:51 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2026-01-06 21:39:51 | INFO | train | epoch 002 | lr 1e-07 | loss_scale 2 | train_wall 1 | wall 61
2026-01-06 21:39:51 | INFO | fairseq.trainer | begin training epoch 3
2026-01-06 21:39:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0
2026-01-06 21:39:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5
2026-01-06 21:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:39:54 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 19.95 | nll_loss 18.636 | ppl 407437 | wps 11489.2 | wpb 1513.5 | bsz 50 | num_updates 1
2026-01-06 21:39:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2026-01-06 21:39:54 | INFO | train | epoch 003 | loss 21.396 | nll_loss 20.116 | ppl 1.13598e+06 | wps 0 | ups 0 | wpb 1524 | bsz 36 | num_updates 1 | lr 3.4975e-07 | gnorm 254.864 | clip 100 | loss_scale 0.5 | train_wall 1 | wall 64
2026-01-06 21:39:54 | INFO | fairseq.trainer | begin training epoch 4
2026-01-06 21:39:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25
2026-01-06 21:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:39:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 19.778 | nll_loss 18.461 | ppl 360718 | wps 9506.3 | wpb 1513.5 | bsz 50 | num_updates 3
2026-01-06 21:39:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2026-01-06 21:39:57 | INFO | train | epoch 004 | loss 20.254 | nll_loss 18.992 | ppl 521515 | wps 999 | ups 0.65 | wpb 1529.5 | bsz 55.5 | num_updates 3 | lr 8.4925e-07 | gnorm 238.671 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 67
2026-01-06 21:39:57 | INFO | fairseq.trainer | begin training epoch 5
2026-01-06 21:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:00 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 17.385 | nll_loss 16.032 | ppl 67001.9 | wps 9288.5 | wpb 1513.5 | bsz 50 | num_updates 6
2026-01-06 21:40:00 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2026-01-06 21:40:00 | INFO | train | epoch 005 | loss 19.529 | nll_loss 18.251 | ppl 311868 | wps 2050.9 | ups 1.08 | wpb 1895.3 | bsz 66.7 | num_updates 6 | lr 1.5985e-06 | gnorm 214.765 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 69
2026-01-06 21:40:00 | INFO | fairseq.trainer | begin training epoch 6
2026-01-06 21:40:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:03 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 13.123 | nll_loss 11.742 | ppl 3424.18 | wps 9287.7 | wpb 1513.5 | bsz 50 | num_updates 9
2026-01-06 21:40:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2026-01-06 21:40:03 | INFO | train | epoch 006 | loss 17.46 | nll_loss 16.175 | ppl 73966.8 | wps 1929.8 | ups 1.02 | wpb 1895.3 | bsz 66.7 | num_updates 9 | lr 2.34775e-06 | gnorm 200.224 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 72
2026-01-06 21:40:03 | INFO | fairseq.trainer | begin training epoch 7
2026-01-06 21:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.524 | nll_loss 9.063 | ppl 534.76 | wps 10244.7 | wpb 1513.5 | bsz 50 | num_updates 12
2026-01-06 21:40:06 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2026-01-06 21:40:06 | INFO | train | epoch 007 | loss 13.767 | nll_loss 12.462 | ppl 5641.3 | wps 1818.9 | ups 0.96 | wpb 1895.3 | bsz 66.7 | num_updates 12 | lr 3.097e-06 | gnorm 120.731 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 76
2026-01-06 21:40:06 | INFO | fairseq.trainer | begin training epoch 8
2026-01-06 21:40:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:09 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.01 | nll_loss 7.397 | ppl 168.5 | wps 9424.5 | wpb 1513.5 | bsz 50 | num_updates 15
2026-01-06 21:40:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2026-01-06 21:40:09 | INFO | train | epoch 008 | loss 11.03 | nll_loss 9.645 | ppl 800.83 | wps 1840.8 | ups 0.97 | wpb 1895.3 | bsz 66.7 | num_updates 15 | lr 3.84625e-06 | gnorm 96.412 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 79
2026-01-06 21:40:09 | INFO | fairseq.trainer | begin training epoch 9
2026-01-06 21:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:12 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.368 | nll_loss 6.699 | ppl 103.91 | wps 10154 | wpb 1513.5 | bsz 50 | num_updates 18
2026-01-06 21:40:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2026-01-06 21:40:12 | INFO | train | epoch 009 | loss 9.603 | nll_loss 8.105 | ppl 275.29 | wps 1816.2 | ups 0.96 | wpb 1895.3 | bsz 66.7 | num_updates 18 | lr 4.5955e-06 | gnorm 107.457 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 82
2026-01-06 21:40:12 | INFO | fairseq.trainer | begin training epoch 10
2026-01-06 21:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.855 | nll_loss 6.179 | ppl 72.43 | wps 8585 | wpb 1513.5 | bsz 50 | num_updates 21
2026-01-06 21:40:15 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2026-01-06 21:40:15 | INFO | train | epoch 010 | loss 8.813 | nll_loss 7.241 | ppl 151.31 | wps 1765 | ups 0.93 | wpb 1895.3 | bsz 66.7 | num_updates 21 | lr 5.34475e-06 | gnorm 55.909 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 85
2026-01-06 21:40:15 | INFO | fairseq.trainer | begin training epoch 11
2026-01-06 21:40:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:18 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.329 | nll_loss 5.68 | ppl 51.27 | wps 8826.2 | wpb 1513.5 | bsz 50 | num_updates 24
2026-01-06 21:40:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2026-01-06 21:40:18 | INFO | train | epoch 011 | loss 8.091 | nll_loss 6.502 | ppl 90.62 | wps 1864.7 | ups 0.98 | wpb 1895.3 | bsz 66.7 | num_updates 24 | lr 6.094e-06 | gnorm 29.211 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 88
2026-01-06 21:40:18 | INFO | fairseq.trainer | begin training epoch 12
2026-01-06 21:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.722 | nll_loss 5.039 | ppl 32.88 | wps 8761.8 | wpb 1513.5 | bsz 50 | num_updates 27
2026-01-06 21:40:21 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2026-01-06 21:40:21 | INFO | train | epoch 012 | loss 7.449 | nll_loss 5.858 | ppl 58.02 | wps 1956.9 | ups 1.03 | wpb 1895.3 | bsz 66.7 | num_updates 27 | lr 6.84325e-06 | gnorm 16.947 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 91
2026-01-06 21:40:21 | INFO | fairseq.trainer | begin training epoch 13
2026-01-06 21:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.077 | nll_loss 4.302 | ppl 19.72 | wps 8818.1 | wpb 1513.5 | bsz 50 | num_updates 30
2026-01-06 21:40:25 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2026-01-06 21:40:25 | INFO | train | epoch 013 | loss 6.734 | nll_loss 5.085 | ppl 33.95 | wps 1735.2 | ups 0.92 | wpb 1895.3 | bsz 66.7 | num_updates 30 | lr 7.5925e-06 | gnorm 9.838 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 94
2026-01-06 21:40:25 | INFO | fairseq.trainer | begin training epoch 14
2026-01-06 21:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:27 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.66 | nll_loss 3.8 | ppl 13.93 | wps 9405.7 | wpb 1513.5 | bsz 50 | num_updates 33
2026-01-06 21:40:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2026-01-06 21:40:27 | INFO | train | epoch 014 | loss 6.085 | nll_loss 4.346 | ppl 20.34 | wps 2025.6 | ups 1.07 | wpb 1895.3 | bsz 66.7 | num_updates 33 | lr 8.34175e-06 | gnorm 8.07 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 97
2026-01-06 21:40:27 | INFO | fairseq.trainer | begin training epoch 15
2026-01-06 21:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:30 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.216 | nll_loss 3.216 | ppl 9.29 | wps 9472.2 | wpb 1513.5 | bsz 50 | num_updates 36
2026-01-06 21:40:30 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2026-01-06 21:40:30 | INFO | train | epoch 015 | loss 5.539 | nll_loss 3.694 | ppl 12.95 | wps 1880.4 | ups 0.99 | wpb 1895.3 | bsz 66.7 | num_updates 36 | lr 9.091e-06 | gnorm 6.476 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 100
2026-01-06 21:40:30 | INFO | fairseq.trainer | begin training epoch 16
2026-01-06 21:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:33 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.639 | nll_loss 2.55 | ppl 5.86 | wps 9167.6 | wpb 1513.5 | bsz 50 | num_updates 39
2026-01-06 21:40:33 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2026-01-06 21:40:33 | INFO | train | epoch 016 | loss 5.002 | nll_loss 3.044 | ppl 8.25 | wps 1851.2 | ups 0.98 | wpb 1895.3 | bsz 66.7 | num_updates 39 | lr 9.84025e-06 | gnorm 5.838 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 103
2026-01-06 21:40:33 | INFO | fairseq.trainer | begin training epoch 17
2026-01-06 21:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.182 | nll_loss 2.022 | ppl 4.06 | wps 8309 | wpb 1513.5 | bsz 50 | num_updates 42
2026-01-06 21:40:36 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2026-01-06 21:40:36 | INFO | train | epoch 017 | loss 4.471 | nll_loss 2.456 | ppl 5.49 | wps 1904.3 | ups 1 | wpb 1895.3 | bsz 66.7 | num_updates 42 | lr 1.05895e-05 | gnorm 8.361 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 106
2026-01-06 21:40:36 | INFO | fairseq.trainer | begin training epoch 18
2026-01-06 21:40:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:39 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.014 | nll_loss 1.753 | ppl 3.37 | wps 8841.3 | wpb 1513.5 | bsz 50 | num_updates 45
2026-01-06 21:40:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2026-01-06 21:40:40 | INFO | train | epoch 018 | loss 4.078 | nll_loss 1.971 | ppl 3.92 | wps 1847.8 | ups 0.97 | wpb 1895.3 | bsz 66.7 | num_updates 45 | lr 1.13388e-05 | gnorm 6.03 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 109
2026-01-06 21:40:40 | INFO | fairseq.trainer | begin training epoch 19
2026-01-06 21:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.885 | nll_loss 1.655 | ppl 3.15 | wps 9409.6 | wpb 1513.5 | bsz 50 | num_updates 48
2026-01-06 21:40:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2026-01-06 21:40:42 | INFO | train | epoch 019 | loss 3.806 | nll_loss 1.617 | ppl 3.07 | wps 2044.5 | ups 1.08 | wpb 1895.3 | bsz 66.7 | num_updates 48 | lr 1.2088e-05 | gnorm 4.573 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 112
2026-01-06 21:40:42 | INFO | fairseq.trainer | begin training epoch 20
2026-01-06 21:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:45 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.692 | nll_loss 1.576 | ppl 2.98 | wps 8855.8 | wpb 1513.5 | bsz 50 | num_updates 51
2026-01-06 21:40:45 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2026-01-06 21:40:45 | INFO | train | epoch 020 | loss 3.577 | nll_loss 1.442 | ppl 2.72 | wps 1843.6 | ups 0.97 | wpb 1895.3 | bsz 66.7 | num_updates 51 | lr 1.28373e-05 | gnorm 3.809 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 115
2026-01-06 21:40:45 | INFO | fairseq.trainer | begin training epoch 21
2026-01-06 21:40:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:48 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.61 | nll_loss 1.535 | ppl 2.9 | wps 8428.3 | wpb 1513.5 | bsz 50 | num_updates 54
2026-01-06 21:40:48 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2026-01-06 21:40:48 | INFO | train | epoch 021 | loss 3.416 | nll_loss 1.377 | ppl 2.6 | wps 1846.8 | ups 0.97 | wpb 1895.3 | bsz 66.7 | num_updates 54 | lr 1.35865e-05 | gnorm 3.803 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 118
2026-01-06 21:40:49 | INFO | fairseq.trainer | begin training epoch 22
2026-01-06 21:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:52 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.554 | nll_loss 1.412 | ppl 2.66 | wps 9417.1 | wpb 1513.5 | bsz 50 | num_updates 57
2026-01-06 21:40:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2026-01-06 21:40:52 | INFO | train | epoch 022 | loss 3.256 | nll_loss 1.19 | ppl 2.28 | wps 1845 | ups 0.97 | wpb 1895.3 | bsz 66.7 | num_updates 57 | lr 1.43358e-05 | gnorm 2.752 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 121
2026-01-06 21:40:52 | INFO | fairseq.trainer | begin training epoch 23
2026-01-06 21:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.444 | nll_loss 1.362 | ppl 2.57 | wps 12333.3 | wpb 1513.5 | bsz 50 | num_updates 60
2026-01-06 21:40:54 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2026-01-06 21:40:54 | INFO | train | epoch 023 | loss 3.134 | nll_loss 1.041 | ppl 2.06 | wps 2082.5 | ups 1.1 | wpb 1895.3 | bsz 66.7 | num_updates 60 | lr 1.5085e-05 | gnorm 2.556 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 124
2026-01-06 21:40:54 | INFO | fairseq.trainer | begin training epoch 24
2026-01-06 21:40:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:40:57 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.396 | nll_loss 1.33 | ppl 2.51 | wps 9685.7 | wpb 1513.5 | bsz 50 | num_updates 63
2026-01-06 21:40:57 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2026-01-06 21:40:57 | INFO | train | epoch 024 | loss 3.037 | nll_loss 0.998 | ppl 2 | wps 1814.5 | ups 0.96 | wpb 1895.3 | bsz 66.7 | num_updates 63 | lr 1.58343e-05 | gnorm 2.262 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 127
2026-01-06 21:40:57 | INFO | fairseq.trainer | begin training epoch 25
2026-01-06 21:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:00 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.392 | nll_loss 1.343 | ppl 2.54 | wps 8890.2 | wpb 1513.5 | bsz 50 | num_updates 66
2026-01-06 21:41:00 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2026-01-06 21:41:00 | INFO | train | epoch 025 | loss 2.924 | nll_loss 0.866 | ppl 1.82 | wps 1910.2 | ups 1.01 | wpb 1895.3 | bsz 66.7 | num_updates 66 | lr 1.65835e-05 | gnorm 2.306 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 130
2026-01-06 21:41:00 | INFO | fairseq.trainer | begin training epoch 26
2026-01-06 21:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.356 | nll_loss 1.296 | ppl 2.46 | wps 9521.2 | wpb 1513.5 | bsz 50 | num_updates 69
2026-01-06 21:41:03 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2026-01-06 21:41:03 | INFO | train | epoch 026 | loss 2.837 | nll_loss 0.808 | ppl 1.75 | wps 2025.5 | ups 1.07 | wpb 1895.3 | bsz 66.7 | num_updates 69 | lr 1.73328e-05 | gnorm 2.341 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 133
2026-01-06 21:41:03 | INFO | fairseq.trainer | begin training epoch 27
2026-01-06 21:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.302 | nll_loss 1.259 | ppl 2.39 | wps 24965 | wpb 1513.5 | bsz 50 | num_updates 72
2026-01-06 21:41:06 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2026-01-06 21:41:06 | INFO | train | epoch 027 | loss 2.767 | nll_loss 0.699 | ppl 1.62 | wps 1934.3 | ups 1.02 | wpb 1895.3 | bsz 66.7 | num_updates 72 | lr 1.8082e-05 | gnorm 2.629 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 136
2026-01-06 21:41:06 | INFO | fairseq.trainer | begin training epoch 28
2026-01-06 21:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:09 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.301 | nll_loss 1.237 | ppl 2.36 | wps 17537.8 | wpb 1513.5 | bsz 50 | num_updates 75
2026-01-06 21:41:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2026-01-06 21:41:09 | INFO | train | epoch 028 | loss 2.708 | nll_loss 0.668 | ppl 1.59 | wps 1885.1 | ups 0.99 | wpb 1895.3 | bsz 66.7 | num_updates 75 | lr 1.88313e-05 | gnorm 2.052 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 139
2026-01-06 21:41:09 | INFO | fairseq.trainer | begin training epoch 29
2026-01-06 21:41:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:12 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.295 | nll_loss 1.285 | ppl 2.44 | wps 8734.8 | wpb 1513.5 | bsz 50 | num_updates 78
2026-01-06 21:41:12 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2026-01-06 21:41:12 | INFO | train | epoch 029 | loss 2.658 | nll_loss 0.601 | ppl 1.52 | wps 1848.8 | ups 0.98 | wpb 1895.3 | bsz 66.7 | num_updates 78 | lr 1.95805e-05 | gnorm 2.083 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 142
2026-01-06 21:41:12 | INFO | fairseq.trainer | begin training epoch 30
2026-01-06 21:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:15 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.269 | nll_loss 1.242 | ppl 2.36 | wps 26421.6 | wpb 1513.5 | bsz 50 | num_updates 81
2026-01-06 21:41:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2026-01-06 21:41:15 | INFO | train | epoch 030 | loss 2.616 | nll_loss 0.596 | ppl 1.51 | wps 1890.5 | ups 1 | wpb 1895.3 | bsz 66.7 | num_updates 81 | lr 2.03297e-05 | gnorm 2.471 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 145
2026-01-06 21:41:15 | INFO | fairseq.trainer | begin training epoch 31
2026-01-06 21:41:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:41:18 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.273 | nll_loss 1.255 | ppl 2.39 | wps 8940 | wpb 1513.5 | bsz 50 | num_updates 84
2026-01-06 21:41:18 | INFO | fairseq_cli.train | begin save checkpoint
2026-01-06 21:42:33 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/tear/surrogate_model/Weather_1_2025/checkpoint31.best_3.2730.pt (epoch 31 @ 84 updates, score 3.273) (writing took 74.82705700956285 seconds)
2026-01-06 21:42:33 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2026-01-06 21:42:33 | INFO | train | epoch 031 | loss 2.556 | nll_loss 0.512 | ppl 1.43 | wps 73 | ups 0.04 | wpb 1895.3 | bsz 66.7 | num_updates 84 | lr 2.1079e-05 | gnorm 1.817 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 223
2026-01-06 21:42:33 | INFO | fairseq.trainer | begin training epoch 32
2026-01-06 21:42:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2026-01-06 21:42:36 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.237 | nll_loss 1.23 | ppl 2.35 | wps 9263.1 | wpb 1513.5 | bsz 50 | num_updates 87
2026-01-06 21:42:36 | INFO | fairseq_cli.train | begin save checkpoint
