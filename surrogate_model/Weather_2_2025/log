2025-11-25 16:29:32 | INFO | fairseq_cli.train | Namespace(activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_ours_large', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=3, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=8000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, newline_token='\n', no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=16, optimizer='adam', optimizer_overrides='{}', patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/export/data/tlice/schema_data/bart.large/model.pt', return_relative_column_strs=['col_head'], save_dir='/export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2025, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='text', split_token='|', stop_time_hours=0, table_max_columns=20, target_lang='data', task='text_to_table_task', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=True, unconstrained_decoding=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='src/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=400, weight_decay=0.01, zero_sharding='none')
2025-11-25 16:29:32 | INFO | fairseq.tasks.translation | [text] dictionary: 50264 types
2025-11-25 16:29:32 | INFO | fairseq.tasks.translation | [data] dictionary: 50264 types
2025-11-25 16:29:38 | INFO | fairseq.data.data_utils | loaded 100 examples from: /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins/valid.text-data.text
2025-11-25 16:29:38 | INFO | fairseq.data.data_utils | loaded 100 examples from: /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins/valid.text-data.data
2025-11-25 16:29:38 | INFO | src.tasks.text_to_table_task | /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins valid text-data 100 examples
2025-11-25 16:29:46 | INFO | fairseq_cli.train | BARTOurs(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerOursDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerRelativeEmbeddingsDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): RelativeEmbeddingsMultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (relation_k_emb): Embedding(2, 64, padding_idx=0)
          (relation_v_emb): Embedding(2, 64, padding_idx=0)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2025-11-25 16:29:46 | INFO | fairseq_cli.train | task: text_to_table_task (TextToDataTranslationTask)
2025-11-25 16:29:46 | INFO | fairseq_cli.train | model: bart_ours_large (BARTOurs)
2025-11-25 16:29:46 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2025-11-25 16:29:46 | INFO | fairseq_cli.train | num. model params: 406293504 (num. trained: 406293504)
2025-11-25 16:29:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2025-11-25 16:29:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2025-11-25 16:29:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-11-25 16:29:47 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.252 GB ; name = NVIDIA A800 80GB PCIe                   
2025-11-25 16:29:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-11-25 16:29:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-11-25 16:29:47 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2025-11-25 16:29:51 | INFO | fairseq.trainer | loaded checkpoint /export/data/tlice/schema_data/bart.large/model.pt (epoch 41 @ 0 updates)
2025-11-25 16:29:51 | INFO | fairseq.trainer | loading train data for epoch 1
2025-11-25 16:29:51 | INFO | fairseq.data.data_utils | loaded 200 examples from: /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins/train.text-data.text
2025-11-25 16:29:51 | INFO | fairseq.data.data_utils | loaded 200 examples from: /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins/train.text-data.data
2025-11-25 16:29:51 | INFO | src.tasks.text_to_table_task | /export/data/tlice/schema_data/text_to_table_data/Weather_multi_2025_200_2/bins train text-data 200 examples
2025-11-25 16:29:51 | INFO | fairseq.trainer | begin training epoch 1
2025-11-25 16:29:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2025-11-25 16:29:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2025-11-25 16:29:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2025-11-25 16:29:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:29:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 20.322 | nll_loss 19.018 | ppl 530916 | wps 16597.2 | wpb 1560.5 | bsz 50 | num_updates 0
2025-11-25 16:29:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2025-11-25 16:29:56 | INFO | train | epoch 001 | lr 1e-07 | loss_scale 16 | train_wall 2 | wall 8
2025-11-25 16:29:56 | INFO | fairseq.trainer | begin training epoch 2
2025-11-25 16:29:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0
2025-11-25 16:29:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0
2025-11-25 16:29:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0
2025-11-25 16:29:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:29:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 20.322 | nll_loss 19.018 | ppl 530916 | wps 18770 | wpb 1560.5 | bsz 50 | num_updates 0
2025-11-25 16:29:59 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2025-11-25 16:29:59 | INFO | train | epoch 002 | lr 1e-07 | loss_scale 2 | train_wall 1 | wall 12
2025-11-25 16:29:59 | INFO | fairseq.trainer | begin training epoch 3
2025-11-25 16:30:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0
2025-11-25 16:30:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5
2025-11-25 16:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 20.321 | nll_loss 19.017 | ppl 530472 | wps 11304.7 | wpb 1560.5 | bsz 50 | num_updates 1
2025-11-25 16:30:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2025-11-25 16:30:02 | INFO | train | epoch 003 | loss 21.593 | nll_loss 20.256 | ppl 1.25208e+06 | wps 0 | ups 0 | wpb 1327 | bsz 26 | num_updates 1 | lr 3.4975e-07 | gnorm 259.251 | clip 100 | loss_scale 0.5 | train_wall 1 | wall 15
2025-11-25 16:30:02 | INFO | fairseq.trainer | begin training epoch 4
2025-11-25 16:30:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25
2025-11-25 16:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 20.146 | nll_loss 18.839 | ppl 468951 | wps 12773 | wpb 1560.5 | bsz 50 | num_updates 3
2025-11-25 16:30:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2025-11-25 16:30:05 | INFO | train | epoch 004 | loss 19.259 | nll_loss 17.971 | ppl 256949 | wps 1314.4 | ups 0.71 | wpb 1858.5 | bsz 68 | num_updates 3 | lr 8.4925e-07 | gnorm 228.39 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 18
2025-11-25 16:30:05 | INFO | fairseq.trainer | begin training epoch 5
2025-11-25 16:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:08 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 17.673 | nll_loss 16.332 | ppl 82469 | wps 10376.1 | wpb 1560.5 | bsz 50 | num_updates 6
2025-11-25 16:30:08 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2025-11-25 16:30:08 | INFO | train | epoch 005 | loss 19.404 | nll_loss 18.105 | ppl 281963 | wps 1972.3 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 6 | lr 1.5985e-06 | gnorm 230.231 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 20
2025-11-25 16:30:08 | INFO | fairseq.trainer | begin training epoch 6
2025-11-25 16:30:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 13.046 | nll_loss 11.664 | ppl 3244.51 | wps 21205.5 | wpb 1560.5 | bsz 50 | num_updates 9
2025-11-25 16:30:11 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2025-11-25 16:30:11 | INFO | train | epoch 006 | loss 17.041 | nll_loss 15.731 | ppl 54391.2 | wps 1987.4 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 9 | lr 2.34775e-06 | gnorm 204.484 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 23
2025-11-25 16:30:11 | INFO | fairseq.trainer | begin training epoch 7
2025-11-25 16:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:13 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.323 | nll_loss 8.829 | ppl 454.87 | wps 19831.8 | wpb 1560.5 | bsz 50 | num_updates 12
2025-11-25 16:30:13 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2025-11-25 16:30:13 | INFO | train | epoch 007 | loss 13.197 | nll_loss 11.831 | ppl 3642.22 | wps 2017.8 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 12 | lr 3.097e-06 | gnorm 120.818 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 26
2025-11-25 16:30:13 | INFO | fairseq.trainer | begin training epoch 8
2025-11-25 16:30:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.875 | nll_loss 7.238 | ppl 150.94 | wps 15300.2 | wpb 1560.5 | bsz 50 | num_updates 15
2025-11-25 16:30:16 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2025-11-25 16:30:16 | INFO | train | epoch 008 | loss 10.773 | nll_loss 9.327 | ppl 642.4 | wps 1995.5 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 15 | lr 3.84625e-06 | gnorm 92.538 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 29
2025-11-25 16:30:16 | INFO | fairseq.trainer | begin training epoch 9
2025-11-25 16:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.331 | nll_loss 6.656 | ppl 100.83 | wps 17179.6 | wpb 1560.5 | bsz 50 | num_updates 18
2025-11-25 16:30:19 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2025-11-25 16:30:19 | INFO | train | epoch 009 | loss 9.475 | nll_loss 7.929 | ppl 243.68 | wps 1987.7 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 18 | lr 4.5955e-06 | gnorm 75.525 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 32
2025-11-25 16:30:19 | INFO | fairseq.trainer | begin training epoch 10
2025-11-25 16:30:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.83 | nll_loss 6.162 | ppl 71.6 | wps 17197.3 | wpb 1560.5 | bsz 50 | num_updates 21
2025-11-25 16:30:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2025-11-25 16:30:22 | INFO | train | epoch 010 | loss 8.699 | nll_loss 7.119 | ppl 138.99 | wps 1993 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 21 | lr 5.34475e-06 | gnorm 51.945 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 35
2025-11-25 16:30:22 | INFO | fairseq.trainer | begin training epoch 11
2025-11-25 16:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:25 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.325 | nll_loss 5.688 | ppl 51.54 | wps 14744 | wpb 1560.5 | bsz 50 | num_updates 24
2025-11-25 16:30:25 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2025-11-25 16:30:25 | INFO | train | epoch 011 | loss 7.982 | nll_loss 6.403 | ppl 84.62 | wps 2003.7 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 24 | lr 6.094e-06 | gnorm 24.786 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 38
2025-11-25 16:30:25 | INFO | fairseq.trainer | begin training epoch 12
2025-11-25 16:30:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:28 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.702 | nll_loss 5.021 | ppl 32.46 | wps 14908.9 | wpb 1560.5 | bsz 50 | num_updates 27
2025-11-25 16:30:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2025-11-25 16:30:28 | INFO | train | epoch 012 | loss 7.318 | nll_loss 5.724 | ppl 52.86 | wps 1972.1 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 27 | lr 6.84325e-06 | gnorm 14.631 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 41
2025-11-25 16:30:28 | INFO | fairseq.trainer | begin training epoch 13
2025-11-25 16:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.126 | nll_loss 4.377 | ppl 20.78 | wps 15467.6 | wpb 1560.5 | bsz 50 | num_updates 30
2025-11-25 16:30:31 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2025-11-25 16:30:31 | INFO | train | epoch 013 | loss 6.729 | nll_loss 5.068 | ppl 33.55 | wps 1995.7 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 30 | lr 7.5925e-06 | gnorm 127.771 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 43
2025-11-25 16:30:31 | INFO | fairseq.trainer | begin training epoch 14
2025-11-25 16:30:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:33 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.711 | nll_loss 3.917 | ppl 15.1 | wps 28041.4 | wpb 1560.5 | bsz 50 | num_updates 33
2025-11-25 16:30:33 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2025-11-25 16:30:33 | INFO | train | epoch 014 | loss 6.167 | nll_loss 4.456 | ppl 21.95 | wps 1992.8 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 33 | lr 8.34175e-06 | gnorm 10.532 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 46
2025-11-25 16:30:34 | INFO | fairseq.trainer | begin training epoch 15
2025-11-25 16:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.319 | nll_loss 3.411 | ppl 10.63 | wps 17800.7 | wpb 1560.5 | bsz 50 | num_updates 36
2025-11-25 16:30:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2025-11-25 16:30:36 | INFO | train | epoch 015 | loss 5.696 | nll_loss 3.926 | ppl 15.2 | wps 2011.7 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 36 | lr 9.091e-06 | gnorm 23.761 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 49
2025-11-25 16:30:36 | INFO | fairseq.trainer | begin training epoch 16
2025-11-25 16:30:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.929 | nll_loss 2.895 | ppl 7.44 | wps 22481 | wpb 1560.5 | bsz 50 | num_updates 39
2025-11-25 16:30:39 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2025-11-25 16:30:39 | INFO | train | epoch 016 | loss 5.248 | nll_loss 3.358 | ppl 10.25 | wps 2013.6 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 39 | lr 9.84025e-06 | gnorm 10.747 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 52
2025-11-25 16:30:39 | INFO | fairseq.trainer | begin training epoch 17
2025-11-25 16:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:42 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.509 | nll_loss 2.427 | ppl 5.38 | wps 17252.3 | wpb 1560.5 | bsz 50 | num_updates 42
2025-11-25 16:30:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2025-11-25 16:30:42 | INFO | train | epoch 017 | loss 4.747 | nll_loss 2.765 | ppl 6.8 | wps 2027.2 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 42 | lr 1.05895e-05 | gnorm 12.196 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 55
2025-11-25 16:30:42 | INFO | fairseq.trainer | begin training epoch 18
2025-11-25 16:30:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:45 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.171 | nll_loss 2.015 | ppl 4.04 | wps 23517.1 | wpb 1560.5 | bsz 50 | num_updates 45
2025-11-25 16:30:45 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2025-11-25 16:30:45 | INFO | train | epoch 018 | loss 4.331 | nll_loss 2.299 | ppl 4.92 | wps 2052.6 | ups 1.08 | wpb 1905 | bsz 66.7 | num_updates 45 | lr 1.13388e-05 | gnorm 5.447 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 58
2025-11-25 16:30:45 | INFO | fairseq.trainer | begin training epoch 19
2025-11-25 16:30:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:48 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.021 | nll_loss 1.793 | ppl 3.46 | wps 26684.3 | wpb 1560.5 | bsz 50 | num_updates 48
2025-11-25 16:30:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2025-11-25 16:30:48 | INFO | train | epoch 019 | loss 3.985 | nll_loss 1.87 | ppl 3.66 | wps 2040.2 | ups 1.07 | wpb 1905 | bsz 66.7 | num_updates 48 | lr 1.2088e-05 | gnorm 20.393 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 60
2025-11-25 16:30:48 | INFO | fairseq.trainer | begin training epoch 20
2025-11-25 16:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.907 | nll_loss 1.722 | ppl 3.3 | wps 24892.1 | wpb 1560.5 | bsz 50 | num_updates 51
2025-11-25 16:30:50 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2025-11-25 16:30:50 | INFO | train | epoch 020 | loss 3.787 | nll_loss 1.622 | ppl 3.08 | wps 2014.9 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 51 | lr 1.28373e-05 | gnorm 5.225 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 63
2025-11-25 16:30:50 | INFO | fairseq.trainer | begin training epoch 21
2025-11-25 16:30:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:53 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.731 | nll_loss 1.643 | ppl 3.12 | wps 25316.5 | wpb 1560.5 | bsz 50 | num_updates 54
2025-11-25 16:30:53 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2025-11-25 16:30:53 | INFO | train | epoch 021 | loss 3.628 | nll_loss 1.539 | ppl 2.91 | wps 2053.2 | ups 1.08 | wpb 1905 | bsz 66.7 | num_updates 54 | lr 1.35865e-05 | gnorm 12.115 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 66
2025-11-25 16:30:53 | INFO | fairseq.trainer | begin training epoch 22
2025-11-25 16:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:56 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.647 | nll_loss 1.587 | ppl 3 | wps 18872.3 | wpb 1560.5 | bsz 50 | num_updates 57
2025-11-25 16:30:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2025-11-25 16:30:56 | INFO | train | epoch 022 | loss 3.476 | nll_loss 1.452 | ppl 2.73 | wps 2038 | ups 1.07 | wpb 1905 | bsz 66.7 | num_updates 57 | lr 1.43358e-05 | gnorm 14.154 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 69
2025-11-25 16:30:56 | INFO | fairseq.trainer | begin training epoch 23
2025-11-25 16:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:30:59 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.618 | nll_loss 1.479 | ppl 2.79 | wps 19064.3 | wpb 1560.5 | bsz 50 | num_updates 60
2025-11-25 16:30:59 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2025-11-25 16:30:59 | INFO | train | epoch 023 | loss 3.366 | nll_loss 1.319 | ppl 2.5 | wps 2035.8 | ups 1.07 | wpb 1905 | bsz 66.7 | num_updates 60 | lr 1.5085e-05 | gnorm 4.688 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 72
2025-11-25 16:30:59 | INFO | fairseq.trainer | begin training epoch 24
2025-11-25 16:31:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:02 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.519 | nll_loss 1.425 | ppl 2.68 | wps 18381.4 | wpb 1560.5 | bsz 50 | num_updates 63
2025-11-25 16:31:02 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2025-11-25 16:31:02 | INFO | train | epoch 024 | loss 3.238 | nll_loss 1.144 | ppl 2.21 | wps 1995.8 | ups 1.05 | wpb 1905 | bsz 66.7 | num_updates 63 | lr 1.58343e-05 | gnorm 4.585 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 74
2025-11-25 16:31:02 | INFO | fairseq.trainer | begin training epoch 25
2025-11-25 16:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:04 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.456 | nll_loss 1.384 | ppl 2.61 | wps 15285.2 | wpb 1560.5 | bsz 50 | num_updates 66
2025-11-25 16:31:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2025-11-25 16:31:04 | INFO | train | epoch 025 | loss 3.18 | nll_loss 1.144 | ppl 2.21 | wps 2047.8 | ups 1.07 | wpb 1905 | bsz 66.7 | num_updates 66 | lr 1.65835e-05 | gnorm 4.923 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 77
2025-11-25 16:31:05 | INFO | fairseq.trainer | begin training epoch 26
2025-11-25 16:31:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.431 | nll_loss 1.354 | ppl 2.56 | wps 20275.1 | wpb 1560.5 | bsz 50 | num_updates 69
2025-11-25 16:31:07 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2025-11-25 16:31:07 | INFO | train | epoch 026 | loss 3.036 | nll_loss 0.98 | ppl 1.97 | wps 2010.8 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 69 | lr 1.73328e-05 | gnorm 3.225 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 80
2025-11-25 16:31:07 | INFO | fairseq.trainer | begin training epoch 27
2025-11-25 16:31:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:10 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.405 | nll_loss 1.366 | ppl 2.58 | wps 21566.5 | wpb 1560.5 | bsz 50 | num_updates 72
2025-11-25 16:31:10 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2025-11-25 16:31:10 | INFO | train | epoch 027 | loss 2.98 | nll_loss 0.936 | ppl 1.91 | wps 2068.8 | ups 1.09 | wpb 1905 | bsz 66.7 | num_updates 72 | lr 1.8082e-05 | gnorm 3.416 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 83
2025-11-25 16:31:10 | INFO | fairseq.trainer | begin training epoch 28
2025-11-25 16:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:13 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.368 | nll_loss 1.321 | ppl 2.5 | wps 18048 | wpb 1560.5 | bsz 50 | num_updates 75
2025-11-25 16:31:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2025-11-25 16:31:13 | INFO | train | epoch 028 | loss 2.899 | nll_loss 0.876 | ppl 1.84 | wps 1985.9 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 75 | lr 1.88313e-05 | gnorm 15.822 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 86
2025-11-25 16:31:13 | INFO | fairseq.trainer | begin training epoch 29
2025-11-25 16:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.329 | nll_loss 1.251 | ppl 2.38 | wps 22598.3 | wpb 1560.5 | bsz 50 | num_updates 78
2025-11-25 16:31:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2025-11-25 16:31:16 | INFO | train | epoch 029 | loss 2.859 | nll_loss 0.816 | ppl 1.76 | wps 2019 | ups 1.06 | wpb 1905 | bsz 66.7 | num_updates 78 | lr 1.95805e-05 | gnorm 15.697 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 89
2025-11-25 16:31:16 | INFO | fairseq.trainer | begin training epoch 30
2025-11-25 16:31:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.321 | nll_loss 1.292 | ppl 2.45 | wps 19777.4 | wpb 1560.5 | bsz 50 | num_updates 81
2025-11-25 16:31:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-11-25 16:31:19 | INFO | train | epoch 030 | loss 2.783 | nll_loss 0.727 | ppl 1.65 | wps 2049.1 | ups 1.08 | wpb 1905 | bsz 66.7 | num_updates 81 | lr 2.03297e-05 | gnorm 3.262 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 91
2025-11-25 16:31:19 | INFO | fairseq.trainer | begin training epoch 31
2025-11-25 16:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:31:21 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.303 | nll_loss 1.272 | ppl 2.42 | wps 18927.8 | wpb 1560.5 | bsz 50 | num_updates 84
2025-11-25 16:31:21 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:36:58 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint31.best_3.3030.pt (epoch 31 @ 84 updates, score 3.303) (writing took 336.4926060400903 seconds)
2025-11-25 16:36:58 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2025-11-25 16:36:58 | INFO | train | epoch 031 | loss 2.758 | nll_loss 0.747 | ppl 1.68 | wps 16.8 | ups 0.01 | wpb 1905 | bsz 66.7 | num_updates 84 | lr 2.1079e-05 | gnorm 3.442 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 431
2025-11-25 16:36:58 | INFO | fairseq.trainer | begin training epoch 32
2025-11-25 16:36:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:37:01 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.301 | nll_loss 1.263 | ppl 2.4 | wps 25233.3 | wpb 1560.5 | bsz 50 | num_updates 87
2025-11-25 16:37:01 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:38:17 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint32.best_3.3010.pt (epoch 32 @ 87 updates, score 3.301) (writing took 76.69897390715778 seconds)
2025-11-25 16:38:17 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2025-11-25 16:38:17 | INFO | train | epoch 032 | loss 2.688 | nll_loss 0.651 | ppl 1.57 | wps 72 | ups 0.04 | wpb 1905 | bsz 66.7 | num_updates 87 | lr 2.18283e-05 | gnorm 3.925 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 510
2025-11-25 16:38:17 | INFO | fairseq.trainer | begin training epoch 33
2025-11-25 16:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:38:20 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.27 | nll_loss 1.249 | ppl 2.38 | wps 23470.4 | wpb 1560.5 | bsz 50 | num_updates 90
2025-11-25 16:38:20 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:40:42 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint33.best_3.2700.pt (epoch 33 @ 90 updates, score 3.27) (writing took 141.48335454426706 seconds)
2025-11-25 16:40:42 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2025-11-25 16:40:42 | INFO | train | epoch 033 | loss 2.645 | nll_loss 0.602 | ppl 1.52 | wps 39.6 | ups 0.02 | wpb 1905 | bsz 66.7 | num_updates 90 | lr 2.25775e-05 | gnorm 2.55 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 655
2025-11-25 16:40:42 | INFO | fairseq.trainer | begin training epoch 34
2025-11-25 16:40:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:40:45 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.266 | nll_loss 1.263 | ppl 2.4 | wps 21693.5 | wpb 1560.5 | bsz 50 | num_updates 93
2025-11-25 16:40:45 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:42:50 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint34.best_3.2660.pt (epoch 34 @ 93 updates, score 3.266) (writing took 125.72397397458553 seconds)
2025-11-25 16:42:52 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2025-11-25 16:42:52 | INFO | train | epoch 034 | loss 2.603 | nll_loss 0.568 | ppl 1.48 | wps 43.9 | ups 0.02 | wpb 1905 | bsz 66.7 | num_updates 93 | lr 2.33268e-05 | gnorm 2.206 | clip 100 | loss_scale 0.25 | train_wall 0 | wall 785
2025-11-25 16:42:52 | INFO | fairseq.trainer | begin training epoch 35
2025-11-25 16:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:42:55 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.262 | nll_loss 1.262 | ppl 2.4 | wps 22460.9 | wpb 1560.5 | bsz 50 | num_updates 96
2025-11-25 16:42:55 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:44:13 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint35.best_3.2620.pt (epoch 35 @ 96 updates, score 3.262) (writing took 78.29375406168401 seconds)
2025-11-25 16:44:14 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2025-11-25 16:44:14 | INFO | train | epoch 035 | loss 2.57 | nll_loss 0.548 | ppl 1.46 | wps 69.4 | ups 0.04 | wpb 1905 | bsz 66.7 | num_updates 96 | lr 2.4076e-05 | gnorm 2.893 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 867
2025-11-25 16:44:14 | INFO | fairseq.trainer | begin training epoch 36
2025-11-25 16:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:44:17 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.258 | nll_loss 1.242 | ppl 2.37 | wps 21005.9 | wpb 1560.5 | bsz 50 | num_updates 99
2025-11-25 16:44:17 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:45:34 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint36.best_3.2580.pt (epoch 36 @ 99 updates, score 3.258) (writing took 77.24035283736885 seconds)
2025-11-25 16:45:36 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2025-11-25 16:45:36 | INFO | train | epoch 036 | loss 2.528 | nll_loss 0.502 | ppl 1.42 | wps 70.2 | ups 0.04 | wpb 1905 | bsz 66.7 | num_updates 99 | lr 2.48253e-05 | gnorm 3.885 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 949
2025-11-25 16:45:36 | INFO | fairseq.trainer | begin training epoch 37
2025-11-25 16:45:37 | INFO | train_inner | epoch 037:      1 / 3 loss=6.037, nll_loss=4.185, ppl=18.18, wps=200.7, ups=0.11, wpb=1892.5, bsz=65.9, num_updates=100, lr=2.5075e-05, gnorm=41.137, clip=100, loss_scale=0.25, train_wall=20, wall=950
2025-11-25 16:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:45:39 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.227 | nll_loss 1.247 | ppl 2.37 | wps 19706.2 | wpb 1560.5 | bsz 50 | num_updates 102
2025-11-25 16:45:39 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:47:28 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint37.best_3.2270.pt (epoch 37 @ 102 updates, score 3.227) (writing took 109.73516396060586 seconds)
2025-11-25 16:47:30 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2025-11-25 16:47:30 | INFO | train | epoch 037 | loss 2.49 | nll_loss 0.458 | ppl 1.37 | wps 50.2 | ups 0.03 | wpb 1905 | bsz 66.7 | num_updates 102 | lr 2.55745e-05 | gnorm 2.159 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1062
2025-11-25 16:47:30 | INFO | fairseq.trainer | begin training epoch 38
2025-11-25 16:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:47:32 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.287 | nll_loss 1.304 | ppl 2.47 | wps 20473.3 | wpb 1560.5 | bsz 50 | num_updates 105
2025-11-25 16:47:32 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:47:32 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2025-11-25 16:47:32 | INFO | train | epoch 038 | loss 2.47 | nll_loss 0.441 | ppl 1.36 | wps 1979.8 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 105 | lr 2.63238e-05 | gnorm 2.328 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1065
2025-11-25 16:47:33 | INFO | fairseq.trainer | begin training epoch 39
2025-11-25 16:47:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:47:35 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.272 | nll_loss 1.286 | ppl 2.44 | wps 19972.4 | wpb 1560.5 | bsz 50 | num_updates 108
2025-11-25 16:47:35 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:47:37 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2025-11-25 16:47:37 | INFO | train | epoch 039 | loss 2.438 | nll_loss 0.422 | ppl 1.34 | wps 1287.9 | ups 0.68 | wpb 1905 | bsz 66.7 | num_updates 108 | lr 2.7073e-05 | gnorm 2.169 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1070
2025-11-25 16:47:37 | INFO | fairseq.trainer | begin training epoch 40
2025-11-25 16:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:47:40 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.323 | nll_loss 1.356 | ppl 2.56 | wps 19386.9 | wpb 1560.5 | bsz 50 | num_updates 111
2025-11-25 16:47:40 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:47:41 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2025-11-25 16:47:41 | INFO | train | epoch 040 | loss 2.416 | nll_loss 0.388 | ppl 1.31 | wps 1390.8 | ups 0.73 | wpb 1905 | bsz 66.7 | num_updates 111 | lr 2.78223e-05 | gnorm 2.096 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1074
2025-11-25 16:47:41 | INFO | fairseq.trainer | begin training epoch 41
2025-11-25 16:47:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:47:44 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.256 | nll_loss 1.293 | ppl 2.45 | wps 20610.5 | wpb 1560.5 | bsz 50 | num_updates 114
2025-11-25 16:47:44 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:50:32 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint41.best_3.2560.pt (epoch 41 @ 114 updates, score 3.256) (writing took 168.61269022896886 seconds)
2025-11-25 16:50:37 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2025-11-25 16:50:37 | INFO | train | epoch 041 | loss 2.402 | nll_loss 0.397 | ppl 1.32 | wps 32.5 | ups 0.02 | wpb 1905 | bsz 66.7 | num_updates 114 | lr 2.85715e-05 | gnorm 2.114 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1250
2025-11-25 16:50:37 | INFO | fairseq.trainer | begin training epoch 42
2025-11-25 16:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:50:39 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.289 | nll_loss 1.319 | ppl 2.49 | wps 19344.1 | wpb 1560.5 | bsz 50 | num_updates 117
2025-11-25 16:50:39 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:50:40 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2025-11-25 16:50:40 | INFO | train | epoch 042 | loss 2.399 | nll_loss 0.379 | ppl 1.3 | wps 2104.8 | ups 1.1 | wpb 1905 | bsz 66.7 | num_updates 117 | lr 2.93208e-05 | gnorm 3.036 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1252
2025-11-25 16:50:40 | INFO | fairseq.trainer | begin training epoch 43
2025-11-25 16:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:50:42 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.306 | nll_loss 1.381 | ppl 2.6 | wps 20399.3 | wpb 1560.5 | bsz 50 | num_updates 120
2025-11-25 16:50:42 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:50:42 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2025-11-25 16:50:42 | INFO | train | epoch 043 | loss 2.37 | nll_loss 0.351 | ppl 1.28 | wps 2150.9 | ups 1.13 | wpb 1905 | bsz 66.7 | num_updates 120 | lr 3.007e-05 | gnorm 2.113 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1255
2025-11-25 16:50:42 | INFO | fairseq.trainer | begin training epoch 44
2025-11-25 16:50:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:50:45 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.293 | nll_loss 1.333 | ppl 2.52 | wps 27250.9 | wpb 1560.5 | bsz 50 | num_updates 123
2025-11-25 16:50:45 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:50:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2025-11-25 16:50:46 | INFO | train | epoch 044 | loss 2.353 | nll_loss 0.35 | ppl 1.27 | wps 1618.8 | ups 0.85 | wpb 1905 | bsz 66.7 | num_updates 123 | lr 3.08193e-05 | gnorm 2.295 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1258
2025-11-25 16:50:46 | INFO | fairseq.trainer | begin training epoch 45
2025-11-25 16:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:50:48 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.284 | nll_loss 1.343 | ppl 2.54 | wps 27311.6 | wpb 1560.5 | bsz 50 | num_updates 126
2025-11-25 16:50:48 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:50:50 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2025-11-25 16:50:50 | INFO | train | epoch 045 | loss 2.329 | nll_loss 0.313 | ppl 1.24 | wps 1244.5 | ups 0.65 | wpb 1905 | bsz 66.7 | num_updates 126 | lr 3.15685e-05 | gnorm 1.934 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1263
2025-11-25 16:50:50 | INFO | fairseq.trainer | begin training epoch 46
2025-11-25 16:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:50:53 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.252 | nll_loss 1.323 | ppl 2.5 | wps 23713.7 | wpb 1560.5 | bsz 50 | num_updates 129
2025-11-25 16:50:53 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:53:30 | INFO | fairseq_cli.train | saved checkpoint /export/data/tlice/schema_data/text_to_table/Weather_multi_2025_200_2_2025/checkpoint46.best_3.2520.pt (epoch 46 @ 129 updates, score 3.252) (writing took 157.63705564290285 seconds)
2025-11-25 16:53:32 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2025-11-25 16:53:32 | INFO | train | epoch 046 | loss 2.316 | nll_loss 0.305 | ppl 1.24 | wps 35.4 | ups 0.02 | wpb 1905 | bsz 66.7 | num_updates 129 | lr 3.23178e-05 | gnorm 3.354 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1424
2025-11-25 16:53:32 | INFO | fairseq.trainer | begin training epoch 47
2025-11-25 16:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-11-25 16:53:34 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.267 | nll_loss 1.347 | ppl 2.54 | wps 28227.3 | wpb 1560.5 | bsz 50 | num_updates 132
2025-11-25 16:53:34 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2025-11-25 16:53:34 | INFO | fairseq_cli.train | begin save checkpoint
2025-11-25 16:53:35 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2025-11-25 16:53:35 | INFO | train | epoch 047 | loss 2.293 | nll_loss 0.285 | ppl 1.22 | wps 1978.2 | ups 1.04 | wpb 1905 | bsz 66.7 | num_updates 132 | lr 3.3067e-05 | gnorm 1.603 | clip 100 | loss_scale 0.25 | train_wall 1 | wall 1427
2025-11-25 16:53:35 | INFO | fairseq_cli.train | done training in 1423.2 seconds
